{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOIr8h0va2ksAW0E8+471vo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"orj3_crF0xSz"},"outputs":[],"source":["pip install langchain-community langchain_groq newspaper3k requests google duckduckgo-search-api python-dotenv lxml_html_clean faiss-cpu duckduckgo_search hf_xet"]},{"cell_type":"code","source":["#-------------------------PART 4: Interacting with the VectorStores and Online news articles----------------------------------\n","import os\n","import requests\n","from typing import List\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.schema import Document\n","from langchain.chains import RetrievalQA\n","from langchain_core.prompts import PromptTemplate\n","from langchain_groq import ChatGroq\n","from concurrent.futures import ThreadPoolExecutor\n","import time\n","import google.generativeai as genai\n","import math\n","import numpy as np\n","from newspaper import Article\n","from transformers import pipeline\n","import faiss\n","from duckduckgo_search import DDGS\n","from newspaper import Article\n","from langchain.docstore.document import Document\n","import shutil\n","import tempfile\n","import pandas as pd\n","import zipfile\n","import warnings\n","from langchain.chat_models import ChatOpenAI"],"metadata":{"id":"WzFWI1ic05j5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set your Gemini API key securely\n","genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n","# Load Gemini model\n","gemini_model = genai.GenerativeModel(\"gemini-1.5-pro\")"],"metadata":{"id":"2a5U1L0b07qb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Summarizer pipeline\n","summarizer = pipeline(\"summarization\")"],"metadata":{"id":"DcV78lSL0_Sq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom normalization for known edge cases\n","def normalize_country_name(name):\n","\n","    SPECIAL_CASES = {\n","    \"gambia, the\": \"gambia\",\n","    \"iran, islamic rep.\": \"iran\",\n","    \"yemen, rep.\": \"yemen\"}\n","\n","    name = name.lower().strip()\n","    name = SPECIAL_CASES.get(name, name)\n","    return name"],"metadata":{"id":"loO2LidJ1Ayn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- Load Country Mapping --------------------\n","def get_country_code_mapping():\n","    url = \"https://api.worldbank.org/v2/sources/6/country?per_page=300&format=JSON\"\n","    response = requests.get(url).json()\n","    rows = response[\"source\"][0][\"concept\"][0][\"variable\"]\n","    return {normalize_country_name(item[\"value\"].lower()): item[\"id\"] for item in rows}  # name: code"],"metadata":{"id":"o32Rojk51Cix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- Extract Articles with Newspaper3k --------------------\n","def extract_article_content(url: str) -> str:\n","    \"\"\"Fetch and extract the content of an article.\"\"\"\n","    try:\n","        article = Article(url)\n","        article.download()\n","        article.parse()\n","        return article.text\n","    except Exception as e:\n","        return f\"Error extracting article: {e}\""],"metadata":{"id":"0goPrflF1Enw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- Fetch Top News Articles --------------------\n","def fetch_articles(country: str, query: str = \"\") -> list:\n","    \"\"\"Uses DuckDuckGo to search for news articles related to a country and extracts the top 10 articles.\"\"\"\n","    search_query = f\"{country} economy debt {query}\".strip()\n","    articles = []\n","\n","    with DDGS() as ddgs:\n","        results = ddgs.text(search_query, max_results=15)\n","        for result in results:\n","            url = result.get(\"href\") or result.get(\"url\")\n","            if url:\n","                content = extract_article_content(url)\n","                if content and not content.startswith(\"Error\"):\n","                    articles.append(content)\n","            if len(articles) >= 5:\n","                break\n","\n","    return articles"],"metadata":{"id":"sjy1U_ud1GZH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-----------------Create Temporary Vectorstore for the news articles-----------------------------------\n","def create_temp_vectorstore_from_news(news_articles: list, embedding_model) -> FAISS:\n","    \"\"\"\n","    Takes a list of news article texts, converts them into LangChain Documents,\n","    and builds a temporary FAISS vectorstore.\n","    \"\"\"\n","    try:\n","        documents = [Document(page_content=article) for article in news_articles if article.strip()]\n","        if not documents:\n","            raise ValueError(\"No valid article content found.\")\n","\n","        temp_vectorstore = FAISS.from_documents(documents, embedding_model)\n","        return temp_vectorstore\n","    except Exception as e:\n","        print(f\"‚ùå Error creating temporary vectorstore: {e}\")\n","        return None"],"metadata":{"id":"x_YfyViX1ILL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- Summarize Articles --------------------\n","def summarize_text(text: str) -> str:\n","    \"\"\"Summarize the text using Hugging Face's summarization pipeline.\"\"\"\n","    summary = summarizer(text, max_length=150, min_length=50, do_sample=False)\n","    return summary[0]['summary_text']"],"metadata":{"id":"PvaSA2La1J2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- Extract Mentioned Country codes from Query --------------------\n","def extract_countries_from_query(query: str, mapping: dict) -> List[str]:\n","    mentioned = []\n","    for name in mapping:\n","        if name in query.lower():\n","            mentioned.append(mapping[name])\n","    return mentioned"],"metadata":{"id":"72CCGN8q1MGB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- Extract Mentioned Country names from Query --------------------\n","def extract_countries_from_query_news(query: str, mapping: dict) -> List[str]:\n","    mentioned = []\n","    for name in mapping:\n","        if name in query.lower():\n","            mentioned.append(name)\n","    return mentioned"],"metadata":{"id":"FYByvSXb1Nkk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#----------------Summarizing using Gemini model---------------------------------------------------------------------------\n","def summarize_with_gemini(context: str, query: str) -> str:\n","    try:\n","        prompt = f\"\"\"\n","You are an expert economic analyst. Based on the data provided below, answer the user's question as accurately as possible. The data may be partial, contradictory, or noisy and may come from multiple sources (e.g., IMF, World Bank, news articles, etc.).\n","\n","Your task is to:\n","- Analyze all available information relevant to the user's query.\n","- If a data source is irrelevant to the question, ignore it completely ‚Äî do not mention it or say that it lacks data.\n","- Provide the best possible or approximate answer along with the reason based on what's available even if the claims are conflicting.\n","- If a definitive answer cannot be given, explain what kind of data would help further ‚Äî but do not blame specific sources.\n","- Ensure your answer is clear, respectful, concise, and well-reasoned.\n","\n","User query:\n","{query}\n","\n","Data:\n","{context}\n","\n","Respond thoughtfully below:\n","\"\"\"\n","        response = gemini_model.generate_content(prompt)\n","        return response.text.strip()\n","    except Exception as e:\n","        return f\"‚ùå Gemini summarization failed: {e}\""],"metadata":{"id":"HkJAnD_L1PA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def summarize_with_gemma(context: str, query: str) -> str:\n","    # Initialize Gemma model via Groq\n","    llm = ChatGroq(\n","    temperature=0,\n","    model_name=\"gemma2-9b-it\",  # or llama3-70b\n","    groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",")\n","    try:\n","        prompt = f\"\"\"\n","You are an expert economic analyst. Based on the data provided below, answer the user's question as accurately as possible. The data may be partial, contradictory, or noisy and may come from multiple sources (e.g., IMF, World Bank, news articles, etc.).\n","\n","Your task is to:\n","- Analyze all available information relevant to the user's query.\n","- If a data source is irrelevant to the question, ignore it completely ‚Äî do not mention it or say that it lacks data.\n","- Provide the best possible or approximate answer along with the reason based on what's available even if the claims are conflicting.\n","- If a definitive answer cannot be given, explain what kind of data would help further ‚Äî but do not blame specific sources.\n","- Ensure your answer is clear, respectful, concise, and well-reasoned.\n","\n","User query:\n","{query}\n","\n","Data:\n","{context}\n","\n","Respond thoughtfully below:\n","\"\"\"\n","        response = llm.invoke(prompt)\n","        return response.content.strip()\n","    except Exception as e:\n","        return f\"‚ùå Gemma summarization failed: {e}\""],"metadata":{"id":"796BXn741Rjr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def summarize_with_openai(context: str, query: str) -> str:\n","    # Initialize OpenAI\n","    llm = ChatOpenAI(\n","    temperature=0,\n","    model_name=\"gpt-4\",\n","    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",")\n","    try:\n","        prompt = f\"\"\"\n","You are an expert economic analyst. Based on the data provided below, answer the user's question as accurately as possible. The data may be partial, contradictory, or noisy and may come from multiple sources (e.g., IMF, World Bank, news articles, etc.).\n","\n","Your task is to:\n","- Analyze all available information relevant to the user's query.\n","- If a data source is irrelevant to the question, ignore it completely ‚Äî do not mention it or say that it lacks data.\n","- Provide the best possible or approximate answer along with the reason based on what's available even if the claims are conflicting.\n","- If a definitive answer cannot be given, explain what kind of data would help further ‚Äî but do not blame specific sources.\n","- Ensure your answer is clear, respectful, concise, and well-reasoned.\n","\n","User query:\n","{query}\n","\n","Data:\n","{context}\n","\n","Respond thoughtfully below:\n","\"\"\"\n","        response = llm.invoke(prompt)\n","        return response.content.strip()\n","    except Exception as e:\n","        return f\"‚ùå OpenAI summarization failed: {e}\""],"metadata":{"id":"il_xjnQ71UZ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- Load World Bank Vectorstore --------------------\n","def load_vectorstore_for_country_code(code: str, embedding_model) -> FAISS:\n","    path = os.path.join(\"/content/vectorstores\", code)\n","    print(f\"üì¶ Looking for vectorstore at: {path}\")\n","    if os.path.exists(path):\n","        print(f\"‚úÖ Found vectorstore for {code}\")\n","        return FAISS.load_local(path, embeddings=embedding_model, allow_dangerous_deserialization=True)\n","    else:\n","        print(f\"‚ùå Vectorstore NOT found for {code}\")\n","        return None"],"metadata":{"id":"m8tWCepX1Xgy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#---------Load the IMF vector store--------------\n","try:\n","    excel_vectorstore = FAISS.load_local(\"/content/imf_excel_vectorstore/content/excel_vectorstore\",HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"),\n","                            allow_dangerous_deserialization=True)\n","    retriever = excel_vectorstore.as_retriever()\n","    llm = ChatGroq(\n","    temperature=0,\n","    model_name=\"gemma2-9b-it\",  # or llama3-70b\n","    groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",")\n","    qa_chain_imf = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n","\n","except Exception as e:\n","        print(f\"‚ùå Error loading vectorstore: {e}\")\n","        excel_vectorstore = None  # or fallback logic"],"metadata":{"id":"vZs4xuQy1ZT8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-----------Loading all the batches of countries--------\n","selected_codes_x = [folder for folder in os.listdir(\"/content/vectorstores\") if os.path.isdir(os.path.join(\"/content/vectorstores\", folder))]\n","\n","# Split countries into smaller batches for parallel processing\n","max_countries_per_batch = 10  # Adjust based on your token limits\n","num_batches = math.ceil(len(selected_codes_x) / max_countries_per_batch)\n","\n","print(f\"üìä Number of batches to process: {num_batches}\")\n","\n","batches = [selected_codes_x[i * max_countries_per_batch: (i + 1) * max_countries_per_batch] for i in range(num_batches)]\n","\n","# Debugging: Print the batches\n","print(f\"Batch split into: {batches}\")"],"metadata":{"id":"wiCMFWJD1a6-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#------------------- Preload all vectorstores for required country codes------------------------\n","from collections import defaultdict\n","batch_vectorstores_dict = defaultdict(list)\n","embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","for i, batch in enumerate(batches):\n","    for country_code in batch:\n","        vectorstore_path = os.path.join(\"/content/vectorstores\", country_code)\n","        if os.path.isdir(vectorstore_path):\n","            try:\n","                vs = load_vectorstore_for_country_code(country_code, embedding_model)\n","                if vs:\n","                    print(f\"‚úÖ Preloaded vectorstore for '{country_code}'\")\n","                    batch_vectorstores_dict[i].append(vs)\n","                else:\n","                    print(f\"‚ùå Failed to load vectorstore for '{country_code}'\")\n","            except Exception as e:\n","                print(f\"‚ùå Error loading vectorstore for '{country_code}': {e}\")"],"metadata":{"id":"pP1Y5zAI1c2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- Process Country Batch --------------------\n","# def process_country_batch(batch_number,countries_batch, query, embedding_model, llm):\n","\n","def process_country_batch(batch_number, query, vectorstores_batches, embedding_model, llm):\n","\n","    # Merge all retrievers\n","    if not vectorstores_batches:\n","        return []\n","\n","    if len(vectorstores_batches) == 1:\n","        retriever = vectorstores_batches[0].as_retriever()\n","    else:\n","        from langchain.retrievers import EnsembleRetriever\n","        retrievers = [vs.as_retriever() for vs in vectorstores_batches]\n","        retriever = EnsembleRetriever(retrievers=retrievers, weights=[1] * len(retrievers))\n","\n","    # Create QA chain\n","    qa_chain = RetrievalQA.from_chain_type(\n","        llm=llm,\n","        retriever=retriever,\n","        return_source_documents=True\n","    )\n","\n","    try:\n","        result = qa_chain.invoke(query)\n","        return result[\"result\"]\n","    except Exception as e:\n","        #print(f\"‚ö†Ô∏è Groq model failed for batch {batch_number}: {e}. Falling back to Gemini summarization.\")\n","        try:\n","            all_docs = []\n","            for vs in vectorstores_batches:\n","                all_docs.extend(vs.similarity_search(\"\", k=10))\n","            combined_context = \"\\n\\n\".join([doc.page_content for doc in all_docs])\n","            return summarize_with_openai(combined_context, query)\n","        except Exception as fallback_error:\n","            return f\"‚ùå Unable to summarize batch due to fallback failure: {fallback_error}\""],"metadata":{"id":"_ff5cAtt1f1G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- Build Hybrid RAG Model with Parallel Processing --------------------\n","def build_parallel_rag_model(query: str):\n","    # Init LLM and embeddings\n","    llm = ChatGroq(\n","    temperature=0,\n","    model_name=\"llama-3.3-70b-versatile\",  # or llama3-70b\n","    groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",")\n","\n","    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","    # Country name ‚Üí code mapping\n","    name_to_code = get_country_code_mapping()\n","\n","    # Define the name you used when saving the vectorstore\n","    vectorstore_name = \"imf_excel_vectorstore\"  # Replace with the actual name you used\n","\n","    # Try to detect mentioned countries\n","    selected_codes = extract_countries_from_query(query, name_to_code)\n","\n","    #Querying the IMF excel dataset\n","    response_imf = qa_chain_imf.invoke(query)\n","    # Check if any documents were actually retrieved\n","    if not response_imf.get(\"source_documents\") or not response_imf[\"source_documents\"]:\n","                answer_imf= \"Source IMF: Nothing\"\n","    else:\n","                answer_imf = \"Source IMF: \" + response_imf[\"result\"]\n","\n","    # If no countries detected, use all\n","    if not selected_codes:\n","        print(\"üîé No countries explicitly found in query. Searching across all vectorstores...\")\n","\n","        selected_codes =  selected_codes_x\n","\n","        # Process each batch in parallel\n","        with ThreadPoolExecutor() as executor:\n","            #futures = [executor.submit(process_country_batch, i, batch, query, embedding_model, llm) for i,batch in enumerate(batches)]\n","            futures = [executor.submit(process_country_batch, i, query, batch_vectorstores_dict[i], embedding_model, llm) for i in range(len(batches))]\n","            results = [future.result() for future in futures]\n","\n","        # Debugging: Check if results are empty\n","        print(f\"Results from batch processing: {results}\")\n","\n","        # Merge results from all batches\n","        merged_results = []\n","        for batch_result in results:\n","            if batch_result:\n","                if isinstance(batch_result, list):\n","                    merged_results.extend(batch_result)\n","                elif isinstance(batch_result, str):\n","                    merged_results.append(batch_result)\n","\n","        # If merged results are empty, handle that case\n","        if not merged_results:\n","            return \"‚ùå No results found after processing batches.\"\n","\n","        # Convert merged results to a list of Documents\n","        documents = [Document(page_content=result) for result in merged_results]\n","\n","        # Create a new vectorstore or retriever using these documents\n","        merged_vectorstore = FAISS.from_documents(documents, embedding_model)\n","\n","        # Now process the query on the merged data with retry handling\n","        max_retries = 3\n","        retry_delay = 10  # seconds\n","\n","        for attempt in range(max_retries):\n","            try:\n","                combined_context = \"\\n\\n\".join([r for r in merged_results if isinstance(r, str) and r.strip()])\n","                final_answer = summarize_with_gemini(combined_context, query)\n","                # Combine both answers\n","                combined_final_answer = final_answer + \"\\n\\n\" + answer_imf\n","                break  # Exit loop if successful\n","            except Exception as e:\n","                error_msg = str(e)\n","                if \"rate limit\" in error_msg.lower() or \"Rate limit\" in error_msg:\n","                    wait_time = retry_delay * (attempt + 1)\n","                    print(f\"‚ö†Ô∏è Rate limit hit. Retrying in {wait_time} seconds... (Attempt {attempt + 1}/{max_retries})\")\n","                    time.sleep(wait_time)\n","                else:\n","                    print(f\"‚ùå Error: {error_msg}\")\n","                    final_answer = f\"‚ùå Error: {e}\"\n","                    break\n","\n","        # Always delete the vectorstore to free memory\n","        try:\n","            del merged_vectorstore\n","        except:\n","            pass\n","\n","        return combined_final_answer\n","\n","    else:\n","        # If specific countries are mentioned, proceed normally\n","        vectorstores = [load_vectorstore_for_country_code(code, embedding_model) for code in selected_codes]\n","        vectorstores = [vs for vs in vectorstores if vs is not None]\n","        country_name = extract_countries_from_query_news(query, name_to_code)\n","        news_articles = fetch_articles(country_name)\n","        news_vectorstore = create_temp_vectorstore_from_news(news_articles, embedding_model)\n","\n","        # Add the news vectorstore if it exists\n","        if news_vectorstore:\n","            vectorstores.append(news_vectorstore)\n","\n","        # Merge all retrievers\n","        if not vectorstores:\n","            return \"‚ùå No valid country vectorstores found.\"\n","        elif len(vectorstores) == 1:\n","            retriever = vectorstores[0].as_retriever()\n","        else:\n","             # Merge all vectorstores into one\n","            merged_vectorstore = vectorstores[0]\n","            for vs in vectorstores[1:]:\n","                 merged_vectorstore.merge_from(vs)\n","            retriever = merged_vectorstore.as_retriever()\n","            #from langchain.retrievers import EnsembleRetriever\n","            #retriever = EnsembleRetriever(retrievers=[vs.as_retriever() for vs in vectorstores], weights=[1]*len(vectorstores))\n","\n","        # Create QA chain\n","        qa_chain = RetrievalQA.from_chain_type(\n","            llm=llm,\n","            retriever=retriever,\n","            return_source_documents=True\n","        )\n","\n","        # Process the query and return the result\n","        try:\n","            result = qa_chain.invoke(query)\n","            final_answer = result[\"result\"]\n","            # Combine both answers\n","            combined_final_answer = final_answer + \"\\n\\n\" + answer_imf\n","            return combined_final_answer\n","\n","        except Exception as e:\n","            if hasattr(e, 'response') and e.response.status_code == 429:\n","                partial = getattr(e, 'partial_text', None)\n","                if partial:\n","                    return partial\n","            return f\"‚ùå Error: {e}\""],"metadata":{"id":"beuNdER_1jFB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"RapzF6mK1mhM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------- Chat Interface --------------------\n","if __name__ == \"__main__\":\n","    print(\"üí¨ Ask me anything about countries and their debt and other economic indicators. Type 'exit' to stop.\\n\")\n","    while True:\n","        query = input(\"üß† You: \")\n","        if query.strip().lower() == \"exit\":\n","            print(\"üëã Exiting chat. Have a great day!\")\n","            break\n","        response = build_parallel_rag_model(query)\n","        print(\"ü§ñ Answer:\", response, \"\\n\")"],"metadata":{"id":"t5IxjUTr1oFV"},"execution_count":null,"outputs":[]}]}